env: MontezumaRevenge-v0
stop:
    timesteps_total: 1000000
num_workers: 2
num_cpus_per_worker: 0.5
num_gpus: 0.5
num_gpus_per_worker: 0.25
num_envs_per_worker: 8
batch_mode: complete_episodes
explore: True
exploration_config:
    type: StochasticSampling
model: 
    custom_model: vision_net
    custom_options: {}

danger_loss_coeff: 0.1
danger_reward_coeff: 0.05
gamma_death: 0.9
lambda_death: 1.0
death_reward: 0.0

gamma: 0.99
lambda: 0.95
lr: 0.0006
num_sgd_iter: 2
sgd_minibatch_size: 256
train_batch_size: 4096
# Initial coefficient for KL divergence.
kl_coeff: 0.0
kl_target: 0.01
vf_loss_coeff: 0.45
entropy_coeff: 0.1
clip_param: 0.2
vf_clip_param: 40
# If specified, clip the global norm of gradients by this amount.
grad_clip: 0.5
vf_share_layers: True
# Whether to clip rewards prior to experience postprocessing. Setting to
# None means clip for Atari only.
clip_rewards: null
use_pytorch: False
evaluation_interval: 1
evaluation_num_episodes: 1
monitor: False
evaluation_config:
    explore: True
    monitor: True
    num_envs_per_worker: 1