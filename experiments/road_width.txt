env: SafetyGymWrapper
env_config:
    robot_base: xmls/car.xml
    num_steps: 3000
    robot_locations: [[-1,0]]
    robot_rot: 3.14
    task: goal
    observe_goal_lidar: True
    observe_hazards: True
    constrain_hazards: True
    lidar_max_dist: 10
    lidar_num_bins: 16
    hazards_num: 22
    randomize_layout: True
    hazards_locations: [[-2,-1],[-2,0],[-2,1],[-2,2],[-2,3],[-1,-1],[-1,3],[0,-1],[0,0],[0,1],[0,3],[1,-1],[1,0],[1,1],[1,3],[2,-1],[2,3],[3,-1],[3,0],[3,1],[3,2],[3,3]]
    hazards_size: 0.5
    hazards_keepout: 0.01
    goal_locations: [[2,0]]
    goal_size: 0.3

stop:
    timesteps_total: 10000000
num_workers: 2
num_cpus_per_worker: 0.5
num_gpus: 0.5
num_gpus_per_worker: 0.25
num_envs_per_worker: 8
batch_mode: complete_episodes
explore: True
exploration_config:
    type: StochasticSampling
model: 
    custom_model: vision_net
    custom_options: {}

danger_loss_coeff: 0.1
danger_reward_coeff: 0.1
gamma_death: 0.95
lambda_death: 1.0

use_death_reward: False
death_reward: 0.0
reward_discount_on_death: 0.95


gamma: 0.99
lambda: 0.95
lr: 0.0006
num_sgd_iter: 2
sgd_minibatch_size: 256
train_batch_size: 4096
# Initial coefficient for KL divergence.
kl_coeff: 0.0
kl_target: 0.01
vf_loss_coeff: 0.45
entropy_coeff: 0.1
clip_param: 0.2
vf_clip_param: 40
# If specified, clip the global norm of gradients by this amount.
grad_clip: 0.5
vf_share_layers: True
# Whether to clip rewards prior to experience postprocessing. Setting to
# None means clip for Atari only.
clip_rewards: null
use_pytorch: False
evaluation_interval: 1
evaluation_num_episodes: 1
monitor: False
evaluation_config:
    explore: True
    monitor: True
    num_envs_per_worker: 1
