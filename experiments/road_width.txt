env: SafetyGymWrapper
env_config: {}

stop:
    timesteps_total: 10000000
num_workers: 2
num_cpus_per_worker: 0.5
num_gpus: 0
num_gpus_per_worker: 0
num_envs_per_worker: 2
batch_mode: complete_episodes
explore: True
exploration_config:
    type: StochasticSampling
model: 
    custom_model: simple_fcnet
    custom_options:
        curiosity_encoding_size: 16

danger_loss_coeff: 0.1
danger_reward_coeff: 1
gamma_death: 0.95
lambda_death: 1.0

use_death_reward: False
death_reward: 0.0
reward_discount_on_death: 0.95

use_curiosity: True
curiosity_reward_coeff: 0.1
curiosity_loss_coeff: 1


gamma: 0.99
lambda: 0.95
lr: 0.0006
num_sgd_iter: 2
sgd_minibatch_size: 256
train_batch_size: 4096
# Initial coefficient for KL divergence.
kl_coeff: 0.0
kl_target: 0.01
vf_loss_coeff: 0.45
entropy_coeff: 0.01
clip_param: 0.2
vf_clip_param: 40
# If specified, clip the global norm of gradients by this amount.
grad_clip: 0.5
vf_share_layers: True
# Whether to clip rewards prior to experience postprocessing. Setting to
# None means clip for Atari only.
clip_rewards: null
use_pytorch: False
evaluation_interval: 5
evaluation_num_episodes: 1
monitor: False
evaluation_config:
    explore: True
    monitor: True
    num_envs_per_worker: 1
