env: SafetyGymWrapper
env_config: {}
checkpoint_freq: 10
checkpoint_at_end: True
keep_checkpoints_num: 5
stop:
    timesteps_total: 150000
num_workers: 2
num_cpus_per_worker: 0.5
num_gpus: 0
num_gpus_per_worker: 0
num_envs_per_worker: 2
batch_mode: complete_episodes
explore: True
exploration_config:
    type: StochasticSampling
model: 
    custom_model: simple_fcnet
    custom_options:
        curiosity_encoding_size: 16
    free_log_std: True

danger_loss_coeff: 0.1
danger_reward_coeff: 5
gamma_death: 0.95
lambda_death: 1.0

use_death_reward: True
death_reward: -1.0
reward_discount_on_death: 0.95

use_curiosity: False
curiosity_reward_coeff: 0.1
curiosity_loss_coeff: 1

ext_reward_coeff: 1.0
ext_reward_coeff_schedule: null

fixed_death_cost_multiplier: 10

gamma: 0.995
lambda: 0.97
lr: 0.0003
num_sgd_iter: 10
sgd_minibatch_size: 64
train_batch_size: 2048
# Initial coefficient for KL divergence.
kl_coeff: 0.0
kl_target: 0.01
vf_loss_coeff: 0.45
entropy_coeff: 0.0
clip_param: 0.2
vf_clip_param: 40
# If specified, clip the global norm of gradients by this amount.
grad_clip: 0.5
vf_share_layers: True
# Whether to clip rewards prior to experience postprocessing. Setting to
# None means clip for Atari only.
clip_rewards: null
use_pytorch: False
evaluation_interval: 5
evaluation_num_episodes: 1
monitor: False
evaluation_config:
    explore: True
    monitor: True
    num_envs_per_worker: 1
